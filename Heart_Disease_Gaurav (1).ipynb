# Importing necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Import classifiers
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC

# Load the dataset
df = pd.read_csv('/content/heart_disease.csv')

df.head()

# Check for missing values
missing_values = df.isnull().sum()
print("Missing Values in Each Column:\n", missing_values)

# Check the total number of missing values
total_missing = missing_values.sum()
print(f"\nTotal Missing Values: {total_missing}")

print('Shape of the data is ', df.shape)

# Check the unique values and their counts
print("Target Variable Distribution:")
print(df['target'].value_counts())

# Percentage distribution
print("\nPercentage Distribution:")
print(df['target'].value_counts(normalize=True) * 100)

# Plot the target variable distribution
plt.figure(figsize=(6,4))
sns.countplot(x=df['target'], palette='pastel')
plt.title("Target Variable Distribution")
plt.xlabel("Heart Disease (0 = No, 1 = Yes)")
plt.ylabel("Count")
plt.show()

# Selecting numerical columns
numerical_features = df.select_dtypes(include=['int64', 'float64']).columns
print("Numerical Features:", numerical_features)
# Plot histograms for numerical features
df[numerical_features].hist(figsize=(12, 10), bins=20, color='skyblue', edgecolor='black')
plt.suptitle("Distribution of Numerical Features")
plt.show()

df[numerical_features].describe().T
# Identify numerical columns
numerical_features = df.select_dtypes(include=['int64', 'float64']).columns

# Compute IQR for each numerical column
Q1 = df[numerical_features].quantile(0.25)  # First quartile (25th percentile)
Q3 = df[numerical_features].quantile(0.75)  # Third quartile (75th percentile)
IQR = Q3 - Q1  # Interquartile Range

# Calculate lower and upper bounds for outlier detection
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Impute outliers with median
for feature in numerical_features:
    median = df[feature].median()
    df[feature] = np.where((df[feature] < lower_bound[feature]) | (df[feature] > upper_bound[feature]), median, df[feature])

df.info()
df.head()
df.tail()

plt.figure(figsize=(12, 10)) # Increased figure height to accommodate more subplots
for i, col in enumerate(numerical_features):
    plt.subplot(4, 4, i + 1)  # Adjusted grid to 4 rows x 4 columns
    sns.boxplot(y=df[col], color='skyblue')
    plt.title(f'Boxplot of {col}')
    plt.xlabel('')
plt.tight_layout()
plt.show()

# Compute the correlation matrix
correlation_matrix = df.corr()
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='crest', fmt=".2f", linewidths=0.5, linecolor='black')
plt.title('Correlation Heatmap of Features')
plt.show()

from sklearn.preprocessing import OneHotEncoder

# Assuming 'data' is your DataFrame
categorical_features = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']

encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
encoded_data = encoder.fit_transform(df[categorical_features])

encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_features))

final_df = pd.concat([df.drop(categorical_features, axis=1), encoded_df], axis=1)


final_df.head()
final_df.describe()

X = final_df.drop('target', axis=1)  # Features
y = final_df['target']  # Target variable

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale numerical features using StandardScaler
numerical_cols = X.select_dtypes(include=np.number).columns
scaler = StandardScaler()
X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])
X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score
# Import roc_curve as well if you are using it for plotting ROC curves
from sklearn.metrics import roc_curve
# Step 1: Standardizing numerical features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)  # Assuming X contains the features
y = df['target']  # Assuming 'target' is the label column

# Step 2: Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)

# Step 3: Define classifiers
classifiers = [
    ('Logistic Regression', LogisticRegression(max_iter=1000)),
    ('Decision Tree', DecisionTreeClassifier(random_state=42)),
    ('Random Forest', RandomForestClassifier(random_state=42)),
    ('Gradient Boosting', GradientBoostingClassifier(random_state=42)),
    ('Support Vector Machine', SVC(probability=True, random_state=42))
]

# Step 4: Store evaluation results
results = []
plt.figure(figsize=(10, 6))  # Prepare for ROC curve plotting

# Step 5: Train and evaluate each model
for name, clf in classifiers:
    # Train model
    clf.fit(X_train, y_train)

    # Predictions
    y_pred = clf.predict(X_test)
    y_prob = clf.predict_proba(X_test)[:, 1]  # Probability for positive class (1)

    # Compute Metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_prob)

    # Store results
    results.append([name, accuracy, precision, recall, f1, roc_auc])

    # Print Classification Report & Confusion Matrix
    print(f"{name} Accuracy: {accuracy:.4f}")
    print(classification_report(y_test, y_pred))
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
    print("-" * 50)

    # Compute and plot ROC Curve
    fpr, tpr, _ = roc_curve(y_test, y_prob)
    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')

# Step 6: Display ROC curves
plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier (AUC = 0.50)')  # Diagonal reference line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

# Step 7: Convert results into DataFrame
results_df = pd.DataFrame(results, columns=['Classifier', 'Accuracy', 'Precision', 'Recall', 'F1-score', 'AUC-ROC'])

# Step 8: Find the best model based on accuracy
best_model = results_df.loc[results_df['Accuracy'].idxmax()]
print(f"\nBest Model: {best_model['Classifier']} with Accuracy: {best_model['Accuracy']:.4f}")

# Step 9: Plot accuracy comparison
plt.figure(figsize=(10, 5))
sns.barplot(x='Classifier', y='Accuracy', data=results_df, palette='coolwarm')
plt.xlabel('Classifier')
plt.ylabel('Accuracy')
plt.title('Model Performance Comparison (Accuracy)')
plt.xticks(rotation=30)
plt.ylim(0, 1)
plt.show()

# Step 10: Display the final results table
print("\nFinal Model Performance Comparison:\n", results_df)

# Iterate through classifiers and plot confusion matrices
for name, classifier in classifiers.items(): # Changed line to iterate over items
    classifier.fit(X_train, y_train)
    y_pred = classifier.predict(X_test)

    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(5, 3))  # Slightly increased size for better visibility
    sns.heatmap(cm, annot=True, fmt="d", cmap="magma",
                xticklabels=['Predicted 0', 'Predicted 1'],
                yticklabels=['Actual 0', 'Actual 1'],
                cbar=False, linewidths=1, linecolor='black')

    plt.title(f'Confusion Matrix for {name}')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.show()

# Feature Importance for Random Forest
rf_classifier = RandomForestClassifier(random_state=42)
rf_classifier.fit(X_train, y_train)
importances = rf_classifier.feature_importances_
# Get feature names from the original DataFrame 'X' before scaling
feature_names = X.columns  # Changed line to access columns from X
random_forest_importances = pd.Series(importances, index=feature_names)

plt.figure(figsize=(8, 5))
sns.barplot(x=random_forest_importances, y=random_forest_importances.index, palette="viridis")
plt.title("Feature Importances of Random Forest using MDI")
plt.xlabel("Mean Decrease in Impurity")
plt.ylabel("Features")
plt.show()

# Feature Importance for Gradient Boosting
gb_classifier = GradientBoostingClassifier(random_state=42)
gb_classifier.fit(X_train, y_train)
importances = gb_classifier.feature_importances_
# Get feature names from the original DataFrame 'X' before scaling
feature_names = X.columns  # Changed line to access columns from X
gradient_boosting_importances = pd.Series(importances, index=feature_names)

plt.figure(figsize=(8, 5))
sns.barplot(x=gradient_boosting_importances, y=gradient_boosting_importances.index, palette="magma")
plt.title("Feature Importances of Gradient Boosting using MDI")
plt.xlabel("Mean Decrease in Impurity")
plt.ylabel("Features")
plt.show()
